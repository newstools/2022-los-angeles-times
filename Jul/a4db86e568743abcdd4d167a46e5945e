Carver Mead isn’t impressed by complicated things. As far as he’s concerned, the bigger challenge is to take a complex system and find a way to simplify it without overlooking any of its essential features. In an era when integrated circuits for computers were painstakingly painted by hand by skilled lithographers, the microelectronics pioneer at Caltech designed a blueprint that made it easy for anyone to place thousands of transistors on a single microcontroller chip. His early 1970s innovation — called very large scale integration, or VLSI — recently won him the prestigious 2022 Kyoto Prize. VLSI played a pivotal role in the semiconductor revolution. It fueled the exponential rise in the number of transistors that could be placed on a chip, shrinking computing devices while expanding their capabilities. After wizarding the movements of electrons around a microchip, Mead became interested in the fundamental laws of physics that govern their motion. He took it upon himself to reformulate the rules of electricity and magnetism, which are taught now the way they were back when they were proposed by James Clerk Maxwell in 1865.  Drawing on more than a century’s worth of modern physics experiments, Mead devised a more holistic picture of electromagnetic phenomena. His approach is based on quantum physics, which treats electrons, photons and other building blocks of matter as both waves and particles. Mead called the result “collective electrodynamics” and used that term as the title of a “little green book” on the topic that he published in 2001. Now a professor emeritus at Caltech, he continues to work on this and other projects. He spoke with The Times about his journey from computer technology to fundamental physics.   Think of the electron as a wave, with a frequency corresponding to its energy and a wavelength related to its momentum. A superconductor contains a huge density of electrons, coupled with each other so they form a giant collective quantum state called the condensate. It’s like one enormous electron. When we make a wire out of a superconductor, the propagation of the condensate wave along the wire is called electrical current, and the frequency of the condensate wave is called the voltage. The components of electromagnetism are thus quantum in origin.  Quantum physics was not known in Maxwell’s days, so the quantum origin of electromagnetic interactions was not visible. Tragically, electromagnetic theory is still taught the old way.  The importance of the potential. Electrical engineering, which has made our modern world, is built on the notion of potential. Many physicists don’t really understand potential — they think it’s some mathematical trick. But actually, it’s a very, very deep concept. In an electrical circuit, the electron condensate in a wire is like water flowing through a pipe. We call its flow the electric current, and its pressure is called the electric potential, or voltage. For the standard stuff, you get the same answer with both. But there are things my approach makes easier to explain. For example, take quantized flux. That describes how something flows through a region in discrete quantities. In the ‘70s, scientists observed that magnetic flux around a tiny doughnut of a superconductor behaved this way. If you have a bunch of them, you get a permanent magnet. That’s what a permanent magnet is — a bunch of little superconducting loops, one in each atom. And they’re all lined up. Extending this to two magnets, you can just calculate what they do with each other and you get the energy beautifully. By thinking about it as a quantum system, collective electrodynamics gives you the right answer in a more straightforward way than the classical approach. And that’s a deep fundamental thing that you can just measure.  Some have found it very interesting. But looking back on it, the book doesn’t have enough explanation, so people have a very hard time following it. Once or twice a year, I get an email from somebody that says, “I just grabbed what you said in your little green book, and it changed my life.” And then it’ll be silent for another year or two.  Yes, I’m hard at work on that. We’re developing new stuff in physics all the time. Let’s just say, as an approximation, we have a doubling of knowledge every five or 10 years. After a few of those, it’s not going to be possible to educate people anymore, because there’s just too much new stuff. So you really only have two choices. One is that you can just become narrower and narrower, where you learn more and more about less and less until you know everything about nothing. Or you can go back and realize that the new knowledge we’ve got allows an incredibly deeper way of grasping the field, and its conceptual relationships.   It’s almost never true. Most of the stuff that’s happening is not the mainstream zeitgeist at all. It’s what people get creative about and go off and try it, and most of it doesn’t work. Most of the things I’ve done haven’t worked, but occasionally I get on one that does. And it feels really good! I’ve spent a lot of time working on the optimum organization of information systems. The general programmed computer — like your laptop or a smartphone — that we use today is very wasteful of its resources. It does one simple thing, and it uses lots of energy to do each simple thing. We are beginning to develop ways in which you could use silicon technology with transistors to emulate things that the brains of animals do. If you study the nervous systems of animals, the organization is very different from a general-purpose computer, and it’s extraordinarily energy-efficient — our brain only takes about 20 watts to run. Being an emeritus professor allows me the time to think more deeply about things, pursue efforts like the little green book, and wonder about things like what happens in the brain. This interview has been edited for length and clarity.